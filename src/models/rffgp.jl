"""
Random Fourier Features-based Gaussian Process regression with inducing points.
"""
mutable struct RFFGP
    z::Vector{Float64}               # inducing time points
    u::Vector{Float64}               # latent states at inducing time points
    x::Vector{Float64}               # training time points
    y::Vector{Float64}               # observations at training time points
    y_mean::Float64                  # mean of the observations
    y_std::Float64                   # std of the observations
    y_standardized::Vector{Float64}  # standardized observations
    œÉ·µ§::Float64                      # state noise std
    œÉ::Float64                       # observation noise std
    tœÉ::PriorTransformation          # prior transform for œÉ
    k::KernelFunctions.Kernel        # kernel
    n_rff::Int                       # number of random fourier features
    h::RFFBasis                      # random fourier features
    tœï::Vector{PriorTransformation}  # prior transforms for the kernel parameters
    blr::BayesianLinearRegressor     # Bayesian linear regressor
    f::BasisFunctionRegressor        # f = BasisFunctionRegressor(blr, h)
    fz::AbstractGPs.FiniteGP         # fz = f(z)
    f‚Ä≤::BasisFunctionRegressor       # f‚Ä≤ = posterior(fz, u)
    f‚Ä≤x::AbstractGPs.FiniteGP        # f‚Ä≤x = f‚Ä≤(x)
    H::Matrix{Float64}               # random fourier features at inducing points
    H‚Ä≤::Matrix{Float64}              # random fourier features at training points
    w::Vector{Float64}               # weights of random fourier features
    dHdt::Matrix{Float64}            # gradient of the random fourier features with respect to the input (rows: n_rff, cols: length(z)) 
    standardize::Bool                # whether to standardize the observations
    centralize::Bool                 # whether to centralize the observations

    function RFFGP(
        z::Vector{Float64},
        u::Vector{Float64},
        x::Vector{Float64},
        y::Vector{Float64},
        œÉ·µ§::Float64,
        œÉ::Float64;
        k::KernelFunctions.Kernel,
        n_rff::Int=100,
        standardize::Bool=false,
        centralize::Bool=false
    )

        # Default prior and transform for œÉ. User can change them by calling `set_priortransform_on_œÉ!``
        tœÉ = PriorTransformation(Normal(0, 1), log)
        
        ùìÅ, Œ± = RandomFourierFeatures.spectral_weights(k)  # lengthscale, magnitude
        outer_scaled = ‚àö(2 * Œ±^2 / n_rff)
        input_dims = 1  # input dimension is only time here

        # Prior for random variables to construct random fourier features
        p_œÑ = Uniform(0, 2œÄ)
        p_œâ = RandomFourierFeatures.spectral_distribution(k, input_dims)  # a zero mean iso normal distribution
        # NOTE: in RandomFourierFeatures.jl, the spectral distribution is a zero 
        # mean iso normal distribution. Here, To caluculate the mapping from h and input,
        # input is scaled by lengthscale.
        # This makes same results as where the input is not scaled by lengthscale and
        # the spectral distribution is N(0, 1/lengthscale^2 * I).

        function sample_params()
            œâ = rand(p_œâ, n_rff)  # frequencies
            œÑ = rand(p_œÑ, n_rff)  # phases
            return œâ, œÑ
        end

        # construct random Fourier features
        h = RFFBasis(ùìÅ, outer_scaled, sample_params()..., sample_params)
        tœï = [PriorTransformation(Normal(0, 1), log) for (i, œïi) in enumerate(only_params(k))]

        # construct an approximated GP with random Fourier features
        prior_mw = FillArrays.Zeros(n_rff)  # prior weight mean
        prior_Œõw = Diagonal(FillArrays.Ones(n_rff))  # prior weight precision
        blr = BayesianLinearRegressor(prior_mw, prior_Œõw)
        f = BasisFunctionRegressor(blr, h)
        fz = f(RowVecs(z[:,:]), œÉ·µ§^2)
        f‚Ä≤ = BayesianLinearRegressors.posterior(fz, u)
        f‚Ä≤x = f‚Ä≤(RowVecs(x[:,:]), œÉ^2)
        H = h(RowVecs(z[:,:])).X
        H‚Ä≤ = h(RowVecs(x[:,:])).X
        w = f‚Ä≤.blr.mw
        dHdt = eval_dHdt(h, z)

        if standardize
            y_mean = StatsBase.mean(y)
            y_std = StatsBase.std(y)
            y_standardized = (y .- y_mean) ./ y_std
        elseif centralize
            y_mean = StatsBase.mean(y)
            y_std = 1.0
            y_standardized = y .- y_mean
        else
            y_mean = 0.0
            y_std = 1.0
            y_standardized = y
        end

        return new(z, u, x, y, y_mean, y_std, y_standardized, œÉ·µ§, œÉ, tœÉ, k, 
            n_rff, h, tœï, blr, f, fz, f‚Ä≤, f‚Ä≤x, H, H‚Ä≤, w, dHdt, standardize, centralize)
    end
end

function reconstruct_gp(gp::RFFGP;
    œï::Union{Nothing,Vector{Float64}}=nothing,
    u::Union{Nothing,Vector{Float64}}=nothing,
    œÉ::Union{Nothing,Float64}=nothing
)
    _œï = isnothing(œï) ? only_params(gp.k) : œï
    _u = isnothing(u) ? gp.u : u
    _œÉ = isnothing(œÉ) ? gp.œÉ : œÉ

    if isnothing(œï) && isnothing(u) && !isnothing(œÉ)
        gp.œÉ = _œÉ
        gp.f‚Ä≤x = gp.f‚Ä≤(RowVecs(gp.x[:,:]), gp.œÉ^2)
    elseif isnothing(œï) && !isnothing(u)
        gp.f‚Ä≤ = BayesianLinearRegressors.posterior(gp.fz, _u)
        gp.œÉ = _œÉ
        gp.f‚Ä≤x = gp.f‚Ä≤(RowVecs(gp.x[:,:]), gp.œÉ^2)
        gp.w = gp.f‚Ä≤.blr.mw
    elseif !isnothing(œï)
        gp = RFFGP(gp.z, _u, gp.x, gp.y, gp.œÉ·µ§, _œÉ;
            k=reconstruct_kernel(gp.k, _œï), 
            n_rff=gp.n_rff, 
            standardize=gp.standardize, 
            centralize=gp.centralize)
    else
        @error "There is no update in the GP."
    end
    return gp
end

function reconstruct_gp(gp::Vector{RFFGP}; œï::AbstractMatrix{<:Real})
    gp_reconstructed = RFFGP[]
    for (gpk, œïk) in zip(gp, eachcol(œï))
        push!(gp_reconstructed, reconstruct_gp(gpk, œï=œïk[:]))
    end
    return gp_reconstructed
end

function set_œÉ_prior!(gp::RFFGP, pœÉ::Distribution)
    gp.pœÉ = pœÉ
end

function f_conditional(f::BasisFunctionRegressor, z::AbstractVector{<:Real}, u::AbstractVector{<:Real})
    fz = f(RowVecs(z[:,:]), 1e-6)
    f‚Ä≤ = BayesianLinearRegressors.posterior(fz, u)
    return f‚Ä≤
end

function update_u!(gp::RFFGP, u::Vector{Float64})
    @assert length(u) == length(gp.z)
    gp.u[:] = u
    gp.f‚Ä≤ = BayesianLinearRegressors.posterior(gp.fz, u)
end

function update_y!(gp::RFFGP, y::Vector{Float64})
    @assert length(y) == length(gp.x)
    gp.y[:] = y
    if gp.standardize
        gp.y_mean = StatsBase.mean(y)
        gp.y_std = StatsBase.std(y)
    elseif gp.centralize
        gp.y_mean = StatsBase.mean(y)
        gp.y_std = 1.0
    else
        gp.y_mean = 0.0
        gp.y_std = 1.0
    end
    gp.y_standardized = (y .- gp.y_mean) ./ gp.y_std
end

weight_mean(gp::RFFGP) = gp.f‚Ä≤.blr.mw  # == Œõw‚Åª¬π/œÉ·µ§¬≤ * Œ¶·µÄx
weight_mean(gp::Vector{RFFGP}) = [weight_mean(gpk) for gpk in gp]
weight_mean_matrix(gp::Vector{RFFGP}) = reduce(vcat, weight_mean(gp)')
weight_precision(gp::RFFGP) = gp.f‚Ä≤.blr.Œõw  # == (Œ±¬≤I + Œ¶·µÄŒ¶/œÉ·µ§¬≤)
noise_cov(gp::RFFGP) = gp.fz.Œ£y  # == Œ£x = œÉ·µ§¬≤I
dHdt(gp::RFFGP) = gp.dHdt  # == ‚àÇH/‚àÇt
Hmat(gp::RFFGP, t::AbstractVector{<:Real}) = gp.h(RowVecs(t[:,:])).X  # == H
Hmat(gp::RFFGP) = Hmat(gp, gp.z)

# predict the mean and variance of the GP at the given time points
f_predictive(gp::RFFGP, t_test::AbstractVector{<:Real}, sd::Float64) = gp.f‚Ä≤(RowVecs(t_test[:,:]), sd)

# gradient of the GP function
# numerical_dHdt = t -> ForwardDiff.jacobian(_t -> h(RowVecs(_t)).X, t)
# eval_dHdt(h, t) ‚âà numerical_dHdt(t)
function eval_dHdt(h::RFFBasis, t::AbstractVector{<:Real})
    ùìÅ = h.inner_weights
    outer_scaled = h.outer_weights  # ‚àö(2 * Œ±^2 / n_rff)
    _t = t ./ ùìÅ
    return (outer_scaled * h.œâ ./ ùìÅ .* -sin.(h.œâ .* _t .+ h.œÑ'))' |> Matrix
end

function eval_dhdŒ±(h::RFFBasis, t::AbstractVector{<:Real}, Œ±::AbstractVector{<:Real})
    ùìÅ = h.inner_weights
    _t = t ./ ùìÅ
    n_rff = length(h.œâ)
    return ‚àö(2 / n_rff) .* cos.(h.œâ .* _t .+ h.œÑ')
end  # TODO

function eval_dhdùìÅ(h::RFFBasis, t::AbstractVector{<:Real}, ùìÅ::AbstractVector{<:Real})
    ùìÅ = h.inner_weights
    outer_scaled = h.outer_weights  # ‚àö(2 * Œ±^2 / n_rff)
    _t = t ./ ùìÅ
    return outer_scaled .* (h.œâ .* _t) ./ ùìÅ^2 .* sin.(h.œâ .* _t .+ h.œÑ')
end  # TODO

mean(gp::RFFGP, w::AbstractVector{<:Real}, t::AbstractVector{<:Real}) = Hmat(gp, t) * w
mean(gp::RFFGP) = mean(gp, gp.w, gp.z)
mean(gp::Vector{RFFGP}, W::AbstractMatrix{<:Real}) = [mean(gpk, wk[:], gpk.z) for (gpk, wk) in zip(gp, eachrow(W))]
mean(gp::Vector{RFFGP}, t::AbstractVector{<:Real}) = [mean(gpk, gpk.w, t) for gpk in gp]
W2X(gp::Vector{RFFGP}, W::AbstractMatrix{<:Real}) = reduce(vcat, mean(gp, W)')
t2X(gp::Vector{RFFGP}, t::AbstractVector{<:Real}) = reduce(vcat, mean(gp, t)')
Wt2X(gp::Vector{RFFGP}, W::AbstractMatrix{<:Real}, t::AbstractVector{<:Real}) = reduce(vcat, mean(gp, W, t)')

function calc_y_mean_and_diagcov(gp::RFFGP, w::AbstractVector{<:Real}, œÉ::Real)
    y_mean = gp.H‚Ä≤ * w
    y_cov = Diagonal(œÉ^2 * ones(length(gp.y_standardized)))
    return y_mean, y_cov
end

dfdt_mean(gp::RFFGP, w::AbstractVector{<:Real}) = gp.dHdt' * w
dfdt_mean(gp::RFFGP) = dfdt_mean(gp, gp.w)
dfdt_mean(gp::Vector{RFFGP}, W::AbstractMatrix{<:Real}) = [dfdt_mean(gp[k], wk[:]) for (k, wk) in enumerate(eachrow(W))]

dfdt_cov(gp::RFFGP) = gp.dHdt' * (weight_precision(gp) \ gp.dHdt)
dfdt_cov(gp::Vector{RFFGP}) = [dfdt_cov(gpk) for gpk in gp]

# logpdf functions
logpdf_w(gp::RFFGP) = sum(logpdf.(Normal(0, 1), gp.f‚Ä≤.blr.mw))  # TODO
logpdf_f(gp::RFFGP) = logpdf(gp.fz, gp.y)  # TODO
logpdf_f‚Ä≤(gp::RFFGP, t::AbstractVector{<:Real}, y::AbstractVector{<:Real}, sd::Float64) = logpdf(gp.f‚Ä≤(RowVecs(t[:,:]), sd), y)  # TODO

# derivatives of the logpdf functions
‚àáw_logpdf_f(gp::RFFGP) = gradlogpdf(gp.f‚Ä≤(RowVecs(gp.z[:,:]), gp.œÉ), gp.y)  # TODO
‚àáw_logpdf_f‚Ä≤(gp::RFFGP, t::AbstractVector{<:Real}, y::AbstractVector{<:Real}, sd::Float64) =
    gradlogpdf(gp.f‚Ä≤(RowVecs(t[:,:]), sd), y)  # TODO

function plot(gp::RFFGP)
    times = gp.z
    diff_t = times[end]-times[1]
    t_test = collect((times[1]-diff_t/20):diff_t/100:(times[end]+diff_t/20))
    Plots.plot(t_test, gp.f‚Ä≤(RowVecs(t_test[:,:]), gp.œÉ), ribbon_scale=3, label="f(t)", xlabel="t")
    Plots.scatter!(gp.z, gp.u, c=:blue, label="x")
    Plots.scatter!(gp.z, gp.y, c=:red, label="y")
    Plots.title!("RFF-GP regression")
end

function plot_graddist(gp::RFFGP)
    grad_mean = dfdt_mean(gp)
    œÉ_vec = sqrt.(diag(dfdt_cov(gp)))
    upper = grad_mean .+ 3 * œÉ_vec
    lower = grad_mean .- 3 * œÉ_vec
    Plots.scatter(gp.z, grad_mean, c=:blue, label="ùîº[df/dt]", ms=1, xlabel="t")
    Plots.bar!(gp.z, upper, fillrange=lower, fillalpha=0.5, label="¬±3œÉ", c=:lightblue)
    Plots.hline!([0], c=:black, label="", ls=:dash)
    Plots.title!("Gradient distribution of RFF-GP")
end

function plot_gradcov(gp::RFFGP; clims=(-1., 1.))
    grad_cov = dfdt_cov(gp)
    mid = sum(clims) / 2
    Plots.heatmap(
        grad_cov, y_flip=true, clims=clims,
        c=cgrad([:blue, :white, :red], [clims[1], 0, clims[2]]))
    Plots.title!("Cov[df/dt, df/dt] in RFF-GP")
end

function compare_gradcovelms(gp::GP, rgp::RFFGP)
    gradcov_gp = dfdt_cov(gp)
    gradcov_rgp = dfdt_cov(rgp)
    elm_gradcov_gp = vcat(eachcol(gradcov_gp)...)
    elm_gradcov_rgp = vcat(eachcol(gradcov_rgp)...)

    scatter(
        elm_gradcov_gp, elm_gradcov_rgp,
        label=false,
        xlabel="Cov[df/dt]_i,j in GP",
        ylabel="Cov[df/dt]_i,j in RFF-GP",
    )
    plot!(-1:0.1:1, -1:0.1:1, label=false, ls=:dash, c=:black)
end
